<html><head><title>Digest for Tuesday, May 20, 2025</title><style>body { font-family: sans-serif; margin: 20px; } h1, h2, h3 { color: #333; } .paper, .theme, .outlier { margin-bottom: 20px; padding: 10px; border: 1px solid #eee; border-radius: 5px; } .label { font-weight: bold; } </style></head><body><h1>Digest for Tuesday, May 20, 2025</h1><h2>Key Takeaway</h2><p>Today's papers underscore a pivotal shift towards making AI models more efficient and capable of advanced reasoning, marking significant progress in the deployment and application of AI in both practical and theoretical domains.</p><h2>Top Papers Today</h2><div class="paper"><h3>#1: Grouping First, Attending Smartly: Training-Free Acceleration for   Diffusion Transformers</h3><p><span class="label">Abstract:</span> Diffusion-based Transformers have demonstrated impressive generative capabilities, but their high computational costs hinder practical deployment, for example, generating an $8192\times 8192$ image can take over an hour on an A100 GPU. In this work, we propose GRAT (\textbf{GR}ouping first, \textbf{AT}tending smartly), a training-free attention acceleration strategy for fast image and video generation without compromising output quality. The key insight is to exploit the inherent sparsity in learned attention maps (which tend to be locally focused) in pretrained Diffusion Transformers and leverage better GPU parallelism. Specifically, GRAT first partitions contiguous tokens into non-overlapping groups, aligning both with GPU execution patterns and the local attention structures learned in pretrained generative Transformers. It then accelerates attention by having all query tokens within the same group share a common set of attendable key and value tokens. These key and value tokens are further restricted to structured regions, such as surrounding blocks or criss-cross regions, significantly reducing computational overhead (e.g., attaining a \textbf{35.8$\times$} speedup over full attention when generating $8192\times 8192$ images) while preserving essential attention patterns and long-range context. We validate GRAT on pretrained Flux and HunyuanVideo for image and video generation, respectively. In both cases, GRAT achieves substantially faster inference without any fine-tuning, while maintaining the performance of full attention. We hope GRAT will inspire future research on accelerating Diffusion Transformers for scalable visual generation.</p><p><span class="label">Why This Matters:</span> Presents a novel, training-free acceleration strategy for Diffusion Transformers, crucial for practical deployment in real-world applications by significantly reducing computational costs.</p><p><span class="label">Categories:</span> cs.CV</p></div><div class="paper"><h3>#2: Language Models use Lookbacks to Track Beliefs</h3><p><span class="label">Abstract:</span> How do language models (LMs) represent characters' beliefs, especially when those beliefs may differ from reality? This question lies at the heart of understanding the Theory of Mind (ToM) capabilities of LMs. We analyze Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal mediation and abstraction. We construct a dataset that consists of simple stories where two characters each separately change the state of two objects, potentially unaware of each other's actions. Our investigation uncovered a pervasive algorithmic pattern that we call a lookback mechanism, which enables the LM to recall important information when it becomes necessary. The LM binds each character-object-state triple together by co-locating reference information about them, represented as their Ordering IDs (OIs) in low rank subspaces of the state token's residual stream. When asked about a character's beliefs regarding the state of an object, the binding lookback retrieves the corresponding state OI and then an answer lookback retrieves the state token. When we introduce text specifying that one character is (not) visible to the other, we find that the LM first generates a visibility ID encoding the relation between the observing and the observed character OIs. In a visibility lookback, this ID is used to retrieve information about the observed character and update the observing character's beliefs. Our work provides insights into the LM's belief tracking mechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.</p><p><span class="label">Why This Matters:</span> Offers groundbreaking insights into how language models track beliefs through 'lookback mechanisms', enhancing our understanding of Theory of Mind capabilities in AI, with potential applications in more nuanced natural language understanding and interaction.</p><p><span class="label">Categories:</span> cs.CL</p></div><div class="paper"><h3>#3: Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning</h3><p><span class="label">Abstract:</span> Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.</p><p><span class="label">Why This Matters:</span> Introduces a method to detect and bridge 'Thought Leaps' in mathematical reasoning datasets, improving model learning and generalization, which could revolutionize how AI systems are trained for complex reasoning tasks.</p><p><span class="label">Categories:</span> cs.CL, cs.AI</p></div><div class="paper"><h3>#4: Emerging Properties in Unified Multimodal Pretraining</h3><p><span class="label">Abstract:</span> Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/</p><p><span class="label">Why This Matters:</span> Demonstrates a unified model for multimodal understanding and generation, showcasing emerging capabilities in complex reasoning with practical applications in AI-driven content creation and interaction.</p><p><span class="label">Categories:</span> cs.CV</p></div><div class="paper"><h3>#5: UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal   Understanding and Generation</h3><p><span class="label">Abstract:</span> We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, which significantly boosts UniGen's image generation quality using a simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act as both image generator and verifier at test time, assessing the semantic alignment between a text prompt and its generated image in a step-by-step CoT manner. Trained entirely on open-source datasets across all stages, UniGen achieves state-of-the-art performance on a range of image understanding and generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on DPG-Bench. Through extensive ablation studies, our work provides actionable insights and addresses key challenges in the full life cycle of building unified MLLMs, contributing meaningful directions to the future research.</p><p><span class="label">Why This Matters:</span> Envisions a next-generation search paradigm that integrates user feedback into generative AI search, potentially transforming how search engines evolve and improve over time through fine-grained, process-level user interactions.</p><p><span class="label">Categories:</span> cs.CV</p></div><h2>Emerging Themes</h2><div class="theme"><h3>Accelerating AI with Efficient Strategies</h3><p>Several papers introduce innovative strategies to make AI models more efficient, either through computational shortcuts, leveraging sparsity, or training-free methodologies, significantly advancing the practical deployment of AI systems.</p><p><span class="label">Related Papers:</span> 2</p></div><div class="theme"><h3>Enhancing Understanding through Advanced Reasoning</h3><p>Papers highlight advancements in AI's reasoning capabilities, from tracking beliefs in language models to filling in missing steps in mathematical reasoning, pushing the boundaries of how AI understands and interacts with complex concepts.</p><p><span class="label">Related Papers:</span> 2</p></div><h2>Weird Flex</h2><div class="outlier"><h3>HDO ice detected toward an isolated low-mass protostar with JWST</h3><p><span class="label">Abstract:</span> Water is detected in environments representing every stage of star and solar system formation, but its chemical evolution throughout these stages remains poorly constrained. Deuterium ratios offer a means of probing chemical links between water in different cosmic regions because of their sensitivity to physicochemical conditions. Here, we present the first detection of the 4.1 $\mu$m HDO ice feature with JWST toward L1527 IRS, an isolated low-mass protostar that may eventually grow to a sun-like mass. We measure an ice HDO/H$_{2}$O ratio of 4.4$^{+3.7}_{-1.7}$$\times$10$^{-3}$, where the reported error is dominated by uncertainties in continuum definition and ice band strengths. This fraction is similar to the gas HDO/H$_{2}$O ratios measured in the warm ($>$100 K) inner cores of other low-mass protostellar envelopes and protoplanetary disks found in comparably isolated star-forming regions. Such a similarity tentatively supports the assumption that water vapor detected in these regions is not significantly altered by gas-phase reactions following ice sublimation. It also supports the hypothesis that pre- and protostellar water ice is largely inherited in a chemically unaltered state by outer protoplanetary disks. However, the fraction is a factor of $\sim$4-10 times higher than the gas HDO/H$_{2}$O ratios measured toward comets and low-mass protostars in clustered star-forming regions. This difference may be due to either gas-phase water reprocessing in protostellar envelopes and protoplanetary disks, or differences between prestellar conditions of isolated dense cores and the clustered star-forming regions that are more analogous to the environment in which our Sun formed.</p><p><span class="label">Why It's Interesting:</span> This paper uniquely bridges the gap between astrophysics and AI, using JWST detections to deepen our understanding of the chemical evolution of water in star formation, demonstrating AI's potential in enhancing our understanding of the universe.</p></div></body></html>